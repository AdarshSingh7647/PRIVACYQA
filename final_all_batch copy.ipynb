{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2c6692",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"qwen/qwen3-next-80b-a3b-instruct-maas\"\n",
    "# MODEL_NAME = \"gemini-2.0-flash-001\"\n",
    "MODEL_NAME = \"openai/gpt-oss-120b-maas\"\n",
    "NUM_GPUS = 2\n",
    "RUN_DATASET = ['fir','fir_hash','cfpb']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9434341",
   "metadata": {},
   "source": [
    "# Gemini inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4affc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Silence Python logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# Silence noisy Google / Vertex / gRPC loggers\n",
    "for logger_name in [\n",
    "    \"google\",\n",
    "    \"google.genai\",\n",
    "    \"google.api_core\",\n",
    "    \"google.auth\",\n",
    "    \"google.cloud\",\n",
    "    \"grpc\",\n",
    "    \"absl\",\n",
    "]:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "# Optional: fully disable HTTP request logs\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b3a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from typing import List\n",
    "import time\n",
    "import random\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/scratch/asing725/CSE336/privacy_qa/prompt-qwen-3-9adf83a178fb.json'\n",
    "PROJECT_ID = \"prompt-qwen-3\"\n",
    "LOCATION = \"global\"\n",
    "def generate_single(\n",
    "    prompt: str,\n",
    "    client: genai.Client,\n",
    "    model: str,\n",
    "    config: types.GenerateContentConfig,\n",
    "    max_retries: int = 6,\n",
    "    base_delay: float = 3.0,\n",
    "    max_delay: float = 90.0,\n",
    "):\n",
    "    \"\"\"Generate response for a single prompt with retry + truncated exponential backoff\"\"\"\n",
    "\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=prompt)]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=contents,\n",
    "                config=config,\n",
    "            )\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise  # re-raise on final failure\n",
    "\n",
    "            # truncated exponential backoff + small jitter\n",
    "            delay = min(max_delay, base_delay * (2 ** attempt))\n",
    "            time.sleep(delay)\n",
    "\n",
    "def get_batch_results(prompts: List, max_tokens: int, temperature: float, model_name: str) -> List[str]:\n",
    "    client = genai.Client(\n",
    "        vertexai=True,\n",
    "        location=LOCATION\n",
    "    )\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        # max_output_tokens=max_tokens,\n",
    "    )\n",
    "    results = []\n",
    "    for entry in prompts:\n",
    "        single_response = generate_single(entry, client, model_name, config)\n",
    "        results.append(single_response)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f4f38",
   "metadata": {},
   "source": [
    "# Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2ce1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asing725/.conda/envs/ranker/lib/python3.10/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-02 06:58:34 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 01-02 06:58:34 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asing725/.conda/envs/ranker/lib/python3.10/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/asing725/.conda/envs/ranker/lib/python3.10/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 250\n",
    "batch_size = 10\n",
    "\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List\n",
    "from itertools import islice\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def run_hf_inference(\n",
    "    prompts_dict,\n",
    "    model_name: str,\n",
    "    model_path,\n",
    "    temperature: float = 0.01,\n",
    "    max_tokens: int = MAX_TOKENS,\n",
    "    tensor_parallel_size: int = NUM_GPUS,\n",
    "    **sampling_kwargs):\n",
    "    \"\"\"\n",
    "    Run inference on a dictionary of prompts using HuggingFace Transformers.\n",
    "    \n",
    "    Args:\n",
    "        prompts_dict: Dictionary mapping output_file -> list of prompts\n",
    "        model_name: Name of the model (key in model_configs)\n",
    "        model_path: Path to the model\n",
    "        temperature: Sampling temperature\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        tensor_parallel_size: Number of GPUs to use\n",
    "        **sampling_kwargs: Additional sampling parameters\n",
    "    \n",
    "    Returns:\n",
    "        None (saves results to files)\n",
    "    \"\"\"    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(f\"Model path: {model_path}\")\n",
    "    print(f\"Using {tensor_parallel_size} GPUs\")\n",
    "    if 'llama' in model_name.lower():\n",
    "        llm = LLM(\n",
    "            model=model_path,\n",
    "            dtype=\"auto\",\n",
    "            tensor_parallel_size=NUM_GPUS,\n",
    "            # pipeline_parallel_size=NUM_GPUS,\n",
    "            trust_remote_code=True,\n",
    "            gpu_memory_utilization=0.95,\n",
    "            # max_model_len=50000,\n",
    "        )\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.01,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "        )\n",
    "\n",
    "    # Flatten all prompts and track their sources\n",
    "    all_prompts = []\n",
    "    prompt_to_file = []\n",
    "    \n",
    "    for output_file, prompts in prompts_dict.items():\n",
    "        for prompt in prompts:\n",
    "            all_prompts.append(prompt)\n",
    "            prompt_to_file.append(output_file)\n",
    "    \n",
    "    print(f\"\\nRunning inference on {len(all_prompts)} prompts\")\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    file_results = {output_file: [] for output_file in prompts_dict.keys()}\n",
    "    file_counters = {output_file: 0 for output_file in prompts_dict.keys()}\n",
    "    \n",
    "    # Process prompts in batches\n",
    "    for i in tqdm(range(0, len(all_prompts), batch_size), desc=\"Inference Batches\"):\n",
    "        batch_prompts = all_prompts[i:i + batch_size]\n",
    "        batch_files = prompt_to_file[i:i + batch_size]\n",
    "        \n",
    "        # Generate texts\n",
    "        if 'llama' in model_name.lower():\n",
    "            generated_texts = llm.generate(batch_prompts, sampling_params)\n",
    "        else:\n",
    "            generated_texts = get_batch_results(batch_prompts, max_tokens, temperature, model_name)\n",
    "        \n",
    "        # time.sleep(3)\n",
    "        # Collect results for this batch\n",
    "        for prompt, generated_text, output_file in zip(batch_prompts, generated_texts, batch_files):\n",
    "            if 'llama' in model_name.lower():\n",
    "                result = {\n",
    "                    \"prompt_question_index\": file_counters[output_file],\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": generated_text.outputs[0].text,\n",
    "                    \"prompt_length\": len(generated_text.prompt_token_ids),\n",
    "                    \"response_length\": len(generated_text.outputs[0].token_ids),\n",
    "                    \"model\": model_name,\n",
    "                }\n",
    "            else:\n",
    "                result = {\n",
    "                    \"prompt_question_index\": file_counters[output_file],\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": generated_text.text,\n",
    "                    \"prompt_length\": generated_text.usage_metadata.prompt_token_count,\n",
    "                    \"response_length\": generated_text.usage_metadata.candidates_token_count,\n",
    "                    \"model\": model_name,\n",
    "                    \"raw_response\":str(generated_text),\n",
    "                }\n",
    "            file_results[output_file].append(result)\n",
    "            file_counters[output_file] += 1\n",
    "\n",
    "        # Save results to respective files\n",
    "        for output_file, results in file_results.items():\n",
    "            output_path = Path(output_file)\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "MODELS = {\n",
    "    \"openai/gpt-oss-120b-maas\": \"vertex_api\",\n",
    "    \"gemini-2.0-flash-001\":\"vertex_api\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b\",\n",
    "    \"qwen/qwen3-next-80b-a3b-instruct-maas\": \"vertex_api\",\n",
    "}\n",
    "\n",
    "def process_prompts(base_path: str, model_name: str):\n",
    "    base_dir = Path(base_path)\n",
    "\n",
    "    # Datasets to process\n",
    "    datasets = RUN_DATASET#['fir','fir_hash','cfpb']\n",
    "    \n",
    "    print(f\"Starting to process prompts from: {base_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Dictionary to store all prompts: output_file -> list of prompts\n",
    "    prompts_dict = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_dir = base_dir / dataset\n",
    "        \n",
    "        if not dataset_dir.exists():\n",
    "            print(f\"Skipping {dataset} - directory not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing dataset: {dataset}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get all JSON files\n",
    "        json_files = sorted(dataset_dir.glob(\"prompts*.json\"))\n",
    "        for json_file in json_files:\n",
    "            # if model_name == \"meta-llama/Llama-3.3-70B-Instruct\" and \"cfpb\"==dataset and \"prompts_setup1_k10\" in json_file.name:\n",
    "            #     print(f\"  Skipping file {json_file.name}  for model {model_name} and dataset {dataset}\")\n",
    "            #     continue\n",
    "            all_prompts = []\n",
    "            print(f\"\\nFile: {json_file.name}\")\n",
    "            \n",
    "            # Load JSON file\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle both single object and array\n",
    "            if isinstance(data, dict):\n",
    "                data = [data]\n",
    "            \n",
    "            print(f\"  Found {len(data)} entries\")\n",
    "            \n",
    "            # Process each entry\n",
    "            for idx, entry in enumerate(data, 1):\n",
    "                # Extract required fields                           \n",
    "                prompt = entry['prompt']\n",
    "                setup = entry['setup']\n",
    "                question_id = entry.get('base_question_id')\n",
    "                all_prompts.append(prompt)\n",
    "                \n",
    "                # print(f\"  Entry {idx}/{len(data)}: ID={question_id}, setup={setup}\")\n",
    "            \n",
    "            # Store prompts with their output file path\n",
    "            output_file = str(dataset_dir / json_file.name.replace(\"prompts\", f\"results_{model_name}\"))\n",
    "            prompts_dict[output_file] = all_prompts\n",
    "            # print()\n",
    "    \n",
    "    # Single run_hf_inference call with all prompts\n",
    "    if prompts_dict:\n",
    "        run_hf_inference(prompts_dict, model_path=MODELS[model_name], model_name=model_name)\n",
    "                    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Processing complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f6001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process prompts from: /scratch/asing725/CSE336/privacy_qa/all_prompts\n",
      "================================================================================\n",
      "\n",
      "Processing dataset: fir\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: prompts_setup1_k10.json\n",
      "  Found 1001 entries\n",
      "\n",
      "File: prompts_setup1_k5.json\n",
      "  Found 1001 entries\n",
      "\n",
      "File: prompts_setup2_k10.json\n",
      "  Found 1001 entries\n",
      "\n",
      "File: prompts_setup2_k5.json\n",
      "  Found 1001 entries\n",
      "\n",
      "Processing dataset: fir_hash\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: prompts_setup1_k10.json\n",
      "  Found 570 entries\n",
      "\n",
      "File: prompts_setup1_k5.json\n",
      "  Found 570 entries\n",
      "\n",
      "File: prompts_setup2_k10.json\n",
      "  Found 570 entries\n",
      "\n",
      "File: prompts_setup2_k5.json\n",
      "  Found 570 entries\n",
      "\n",
      "Processing dataset: cfpb\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: prompts_setup1_k10.json\n",
      "  Found 1356 entries\n",
      "\n",
      "File: prompts_setup1_k5.json\n",
      "  Found 1356 entries\n",
      "\n",
      "File: prompts_setup2_k10.json\n",
      "  Found 1356 entries\n",
      "\n",
      "File: prompts_setup2_k5.json\n",
      "  Found 1356 entries\n",
      "Loading model: openai/gpt-oss-120b-maas\n",
      "Model path: vertex_api\n",
      "Using 2 GPUs\n",
      "\n",
      "Running inference on 11708 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f89d4401ab9453bbe3a92062433aaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Batches:   0%|          | 0/1171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_PATH = \"/scratch/asing725/CSE336/privacy_qa/all_prompts\"\n",
    "process_prompts(BASE_PATH, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6ed8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
