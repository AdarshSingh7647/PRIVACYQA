{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a686fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scratch/asing725/Huggingface/hub/models--openai--gpt-oss-120b/snapshots/b5c939de8f754692c1647ca79fbf85e8c1e70f8a/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c1ac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-31 06:53:19 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "NUM_GPUS = 2\n",
    "MAX_TOKENS = 512\n",
    "batch_size = 100\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List\n",
    "from itertools import islice\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def run_hf_inference(\n",
    "    prompts_dict,\n",
    "    model_name: str,\n",
    "    model_path,\n",
    "    temperature: float = 0.01,\n",
    "    max_tokens: int = MAX_TOKENS,\n",
    "    tensor_parallel_size: int = NUM_GPUS,\n",
    "    **sampling_kwargs):\n",
    "    \"\"\"\n",
    "    Run inference on a dictionary of prompts using HuggingFace Transformers.\n",
    "    \n",
    "    Args:\n",
    "        prompts_dict: Dictionary mapping output_file -> list of prompts\n",
    "        model_name: Name of the model (key in model_configs)\n",
    "        model_path: Path to the model\n",
    "        temperature: Sampling temperature\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        tensor_parallel_size: Number of GPUs to use\n",
    "        **sampling_kwargs: Additional sampling parameters\n",
    "    \n",
    "    Returns:\n",
    "        None (saves results to files)\n",
    "    \"\"\"    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(f\"Model path: {model_path}\")\n",
    "    print(f\"Using {tensor_parallel_size} GPUs\")\n",
    "    \n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        dtype=\"auto\",\n",
    "        tensor_parallel_size=NUM_GPUS,\n",
    "        # pipeline_parallel_size=3,\n",
    "        trust_remote_code=True,\n",
    "        gpu_memory_utilization=0.97,\n",
    "        max_model_len=30000,\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.01,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "    # Flatten all prompts and track their sources\n",
    "    all_prompts = []\n",
    "    prompt_to_file = []\n",
    "    \n",
    "    for output_file, prompts in prompts_dict.items():\n",
    "        for prompt in prompts:\n",
    "            all_prompts.append(prompt)\n",
    "            prompt_to_file.append(output_file)\n",
    "    \n",
    "    print(f\"\\nRunning inference on {len(all_prompts)} prompts\")\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    file_results = {output_file: [] for output_file in prompts_dict.keys()}\n",
    "    file_counters = {output_file: 0 for output_file in prompts_dict.keys()}\n",
    "    \n",
    "    # Process prompts in batches\n",
    "    for i in tqdm(range(0, len(all_prompts), batch_size), desc=\"Inference Batches\"):\n",
    "        batch_prompts = all_prompts[i:i + batch_size]\n",
    "        batch_files = prompt_to_file[i:i + batch_size]\n",
    "        \n",
    "        # Generate texts\n",
    "        generated_texts = llm.generate(batch_prompts, sampling_params)\n",
    "        \n",
    "        # Collect results for this batch\n",
    "        for prompt, generated_text, output_file in zip(batch_prompts, generated_texts, batch_files):\n",
    "            result = {\n",
    "                \"prompt_question_index\": file_counters[output_file],\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": generated_text.outputs[0].text,\n",
    "                \"prompt_length\": len(generated_text.prompt_token_ids),\n",
    "                \"response_length\": len(generated_text.outputs[0].token_ids),\n",
    "                \"model\": model_name,\n",
    "            }\n",
    "            file_results[output_file].append(result)\n",
    "            file_counters[output_file] += 1\n",
    "    \n",
    "        # Save results to respective files\n",
    "        for output_file, results in file_results.items():\n",
    "            output_path = Path(output_file)\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# def run_only_hf_inference(\n",
    "#         prompts_dict,\n",
    "#     model_name: str,\n",
    "#     model_path,\n",
    "#     temperature: float = 0.01,\n",
    "#     max_tokens: int = MAX_TOKENS,\n",
    "#     tensor_parallel_size: int = NUM_GPUS,\n",
    "#     **sampling_kwargs):\n",
    "#     \"\"\"\n",
    "#     Run inference on a dictionary of prompts using HuggingFace Transformers.\n",
    "    \n",
    "#     Args:\n",
    "#         prompts_dict: Dictionary mapping output_file -> list of prompts\n",
    "#         model_name: Name of the model (key in model_configs)\n",
    "#         model_path: Path to the model\n",
    "#         temperature: Sampling temperature\n",
    "#         max_tokens: Maximum tokens to generate\n",
    "#         tensor_parallel_size: Number of GPUs to use\n",
    "#         **sampling_kwargs: Additional sampling parameters\n",
    "    \n",
    "#     Returns:\n",
    "#         None (saves results to files)\n",
    "#     \"\"\"    \n",
    "#     import torch\n",
    "#     from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "#     print(f\"Loading model: {model_name}\")\n",
    "#     # print(f\"Model path: {model_path}\")\n",
    "#     print(f\"Using {tensor_parallel_size} GPUs\")\n",
    "    \n",
    "#     # Load model and tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\n",
    "#         model_name,\n",
    "#         trust_remote_code=True\n",
    "#     )\n",
    "    \n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         torch_dtype=\"auto\",\n",
    "#         device_map=\"auto\",\n",
    "#         trust_remote_code=True,\n",
    "#         cache_dir=\"/scratch/asing725/Huggingface/hub\",\n",
    "#         max_memory={\n",
    "#             0: \"75GB\",  # Adjust per your GPU VRAM\n",
    "#             1: \"75GB\",\n",
    "#             2: \"75GB\",\n",
    "#         },\n",
    "#     )\n",
    "    \n",
    "#     # Set padding token if not set\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "#     # Flatten all prompts and track their sources\n",
    "#     all_prompts = []\n",
    "#     prompt_to_file = []\n",
    "    \n",
    "#     for output_file, prompts in prompts_dict.items():\n",
    "#         for prompt in prompts:\n",
    "#             all_prompts.append(prompt)\n",
    "#             prompt_to_file.append(output_file)\n",
    "    \n",
    "#     print(f\"\\nRunning inference on {len(all_prompts)} prompts\")\n",
    "\n",
    "#     # Initialize results dictionary\n",
    "#     file_results = {output_file: [] for output_file in prompts_dict.keys()}\n",
    "#     file_counters = {output_file: 0 for output_file in prompts_dict.keys()}\n",
    "    \n",
    "#     # Process prompts in batches\n",
    "#     for i in tqdm(range(0, len(all_prompts), batch_size), desc=\"Inference Batches\"):\n",
    "#         batch_prompts = all_prompts[i:i + batch_size]\n",
    "#         batch_files = prompt_to_file[i:i + batch_size]\n",
    "        \n",
    "#         # Tokenize batch\n",
    "#         inputs = tokenizer(\n",
    "#             batch_prompts,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             padding_side='left',\n",
    "#             truncation=True,\n",
    "#             max_length=50000 - max_tokens,\n",
    "#         ).to(model.device)\n",
    "        \n",
    "#         # Generate texts\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(\n",
    "#                 **inputs,\n",
    "#                 max_new_tokens=max_tokens,\n",
    "#                 temperature=temperature,\n",
    "#                 do_sample=temperature > 0,\n",
    "#                 pad_token_id=tokenizer.pad_token_id\n",
    "#             )\n",
    "        \n",
    "#         # Decode generated texts\n",
    "#         generated_texts = tokenizer.batch_decode(\n",
    "#             outputs[:, inputs['input_ids'].shape[1]:],\n",
    "#             skip_special_tokens=True\n",
    "#         )\n",
    "        \n",
    "#         # Collect results for this batch\n",
    "#         for prompt, generated_text, output, input_ids, output_file in zip(\n",
    "#             batch_prompts, generated_texts, outputs, inputs['input_ids'], batch_files\n",
    "#         ):\n",
    "#             result = {\n",
    "#                 \"prompt_question_index\": file_counters[output_file],\n",
    "#                 \"prompt\": prompt,\n",
    "#                 \"response\": generated_text,\n",
    "#                 \"prompt_length\": len(input_ids),\n",
    "#                 \"response_length\": len(output) - len(input_ids),\n",
    "#                 \"model\": model_name,\n",
    "#             }\n",
    "#             file_results[output_file].append(result)\n",
    "#             file_counters[output_file] += 1\n",
    "    \n",
    "#         # Save results to respective files\n",
    "#         for output_file, results in file_results.items():\n",
    "#             output_path = Path(output_file)\n",
    "#             output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#             with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#                 json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "#     return\n",
    "\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "MODELS = {\n",
    "    # \"openai/gpt-oss-120b\": \"/scratch/asing725/Huggingface/hub/models--openai--gpt-oss-120b/snapshots/b5c939de8f754692c1647ca79fbf85e8c1e70f8a\",\n",
    "    # \"meta-llama/Llama-3.1-70B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.1-70B-Instruct/snapshots/1605565b47bb9346c5515c34102e054115b4f98b\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b\",\n",
    "    # \"Qwen/Qwen2.5-72B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31\",\n",
    "}\n",
    "\n",
    "def process_prompts(base_path: str, model_name: str):\n",
    "    base_dir = Path(base_path)\n",
    "\n",
    "    # Datasets to process\n",
    "    datasets = ['cfpb', 'fir', 'fir_hash']\n",
    "    \n",
    "    print(f\"Starting to process prompts from: {base_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Dictionary to store all prompts: output_file -> list of prompts\n",
    "    prompts_dict = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_dir = base_dir / dataset\n",
    "        \n",
    "        if not dataset_dir.exists():\n",
    "            print(f\"Skipping {dataset} - directory not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing dataset: {dataset}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get all JSON files\n",
    "        json_files = sorted(dataset_dir.glob(\"*.json\"))\n",
    "        for json_file in json_files:\n",
    "            all_prompts = []\n",
    "            print(f\"\\nFile: {json_file.name}\")\n",
    "            \n",
    "            # Load JSON file\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle both single object and array\n",
    "            if isinstance(data, dict):\n",
    "                data = [data]\n",
    "            \n",
    "            print(f\"  Found {len(data)} entries\")\n",
    "            \n",
    "            # Process each entry\n",
    "            for idx, entry in enumerate(data, 1):\n",
    "                # Extract required fields                           \n",
    "                prompt = entry['prompt']\n",
    "                setup = entry['setup']\n",
    "                question_id = entry.get('base_question_id')\n",
    "                all_prompts.append(prompt)\n",
    "                \n",
    "                # print(f\"  Entry {idx}/{len(data)}: ID={question_id}, setup={setup}\")\n",
    "            \n",
    "            # Store prompts with their output file path\n",
    "            output_file = str(dataset_dir / json_file.name.replace(\"prompts\", f\"results_{model_name}\"))\n",
    "            prompts_dict[output_file] = all_prompts\n",
    "            print()\n",
    "    \n",
    "    # Single run_hf_inference call with all prompts\n",
    "    if prompts_dict:\n",
    "        run_hf_inference(prompts_dict, model_path=MODELS[model_name], model_name=model_name)\n",
    "                    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Processing complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2ce1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# NUM_GPUS = 3\n",
    "# MAX_TOKENS = 512\n",
    "# batch_size = 200\n",
    "\n",
    "# import torch\n",
    "# from datetime import datetime\n",
    "# import gc\n",
    "# from vllm import LLM, SamplingParams\n",
    "# from typing import List\n",
    "# from itertools import islice\n",
    "# import json\n",
    "# import time\n",
    "# from pathlib import Path\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# def run_hf_inference(\n",
    "#     output_file,\n",
    "#     prompts: List[str],\n",
    "#     model_name: str,\n",
    "#     model_path,\n",
    "#     temperature: float = 0.01,\n",
    "#     max_tokens: int = MAX_TOKENS,\n",
    "#     tensor_parallel_size: int = NUM_GPUS,\n",
    "#     **sampling_kwargs):\n",
    "#     \"\"\"\n",
    "#     Run inference on a list of prompts using HuggingFace Transformers.\n",
    "    \n",
    "#     Args:\n",
    "#         prompts: List of input prompts\n",
    "#         model_name: Name of the model (key in model_configs)\n",
    "#         model_configs: Dictionary containing model configurations\n",
    "#         output_file: Optional path to save results as JSON (not used, kept for compatibility)\n",
    "#         temperature: Sampling temperature\n",
    "#         max_tokens: Maximum tokens to generate\n",
    "#         tensor_parallel_size: Number of GPUs to use\n",
    "#         batch_size: Batch size for inference\n",
    "#         **sampling_kwargs: Additional sampling parameters\n",
    "    \n",
    "#     Returns:\n",
    "#         List of dictionaries containing prompts and responses\n",
    "#     \"\"\"    \n",
    "#     print(f\"Loading model: {model_name}\")\n",
    "#     print(f\"Model path: {model_path}\")\n",
    "#     print(f\"Using {tensor_parallel_size} GPUs\")\n",
    "    \n",
    "#     llm = LLM(\n",
    "#         model=model_name,\n",
    "#         dtype=\"auto\",\n",
    "#         # tensor_parallel_size=tensor_parallel_size,\n",
    "#         trust_remote_code=True,\n",
    "#         gpu_memory_utilization=0.95,\n",
    "#         max_model_len=50000,\n",
    "#     )\n",
    "\n",
    "#     sampling_params = SamplingParams(\n",
    "#         temperature=0.01,\n",
    "#         max_tokens=MAX_TOKENS,\n",
    "#     )\n",
    "\n",
    "#     print(f\"\\nRunning inference on {len(prompts)} prompts\")\n",
    "\n",
    "#     results = []\n",
    "    \n",
    "#     # Process prompts in batches\n",
    "#     for i in tqdm(range(0, len(prompts), batch_size),desc=\"Inference Batches\"):\n",
    "#         batch_prompts = prompts[i:i + batch_size]\n",
    "        \n",
    "#         # Tokenize batch\n",
    "#         generated_texts = llm.generate(batch_prompts, sampling_params)\n",
    "#         # Collect results for this batch\n",
    "#         for j, (prompt, generated_text) in enumerate(zip(batch_prompts, generated_texts)):\n",
    "#             result = {\n",
    "#                 \"prompt_question_index\": i + j,\n",
    "#                 \"prompt\": prompt,\n",
    "#                 \"response\": generated_text.outputs[0].text,\n",
    "#                 \"prompt_length\": len(generated_text.prompt_token_ids),\n",
    "#                 \"response_length\": len(generated_text.outputs[0].token_ids),\n",
    "#                 \"model\": model_name,\n",
    "#             }\n",
    "#             results.append(result)\n",
    "#         if output_file:\n",
    "#             output_path = Path(output_file)\n",
    "#             output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#             with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#                 json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "#     return \n",
    "\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# MODELS = {\n",
    "#     # \"openai/gpt-oss-120b\": \"/scratch/asing725/Huggingface/hub/models--openai--gpt-oss-120b/snapshots/b5c939de8f754692c1647ca79fbf85e8c1e70f8a\",\n",
    "#     # \"meta-llama/Llama-3.1-70B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.1-70B-Instruct/snapshots/1605565b47bb9346c5515c34102e054115b4f98b\",\n",
    "#     \"meta-llama/Llama-3.3-70B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b\",\n",
    "#     # \"Qwen/Qwen2.5-72B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31\",\n",
    "# }\n",
    "\n",
    "# def process_prompts(base_path: str, model_name: str):\n",
    "#     base_dir = Path(base_path)\n",
    "#     results = []\n",
    "\n",
    "#     # Datasets to process\n",
    "#     datasets = ['cfpb', 'fir', 'fir_hash']\n",
    "    \n",
    "#     print(f\"Starting to process prompts from: {base_path}\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     for dataset in datasets:\n",
    "#         dataset_dir = base_dir / dataset\n",
    "        \n",
    "#         if not dataset_dir.exists():\n",
    "#             print(f\"Skipping {dataset} - directory not found\")\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"\\nProcessing dataset: {dataset}\")\n",
    "#         print(\"-\" * 80)\n",
    "        \n",
    "#         # Get all JSON files\n",
    "#         json_files = sorted(dataset_dir.glob(\"*.json\"))\n",
    "#         for json_file in json_files:\n",
    "#             all_prompts = []\n",
    "#             print(f\"\\nFile: {json_file.name}\")\n",
    "            \n",
    "#             # Load JSON file\n",
    "#             with open(json_file, 'r') as f:\n",
    "#                 data = json.load(f)\n",
    "            \n",
    "#             # Handle both single object and array\n",
    "#             if isinstance(data, dict):\n",
    "#                 data = [data]\n",
    "            \n",
    "#             print(f\"  Found {len(data)} entries\")\n",
    "            \n",
    "#             # Process each entry\n",
    "#             for idx, entry in enumerate(data, 1):\n",
    "#                 # Extract required fields                           \n",
    "#                 prompt = entry['prompt']\n",
    "#                 setup = entry['setup']\n",
    "#                 question_id = entry.get('base_question_id')\n",
    "#                 all_prompts.append(prompt)\n",
    "                \n",
    "#             print(f\"  Entry {idx}/{len(data)}: ID={question_id}, setup={setup}\")\n",
    "#             run_hf_inference((dataset_dir / (json_file.name).replace(\"prompts\", f\"results_{model_name}\")), all_prompts, model_path=MODELS[model_name], model_name=model_name)\n",
    "                    \n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(f\"Processing complete!\")\n",
    "#     print(\"=\" * 80)\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f6001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process prompts from: /scratch/asing725/CSE336/privacy_qa/all_prompts\n",
      "================================================================================\n",
      "\n",
      "Processing dataset: cfpb\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: prompts_setup1_k10.json\n",
      "  Found 1356 entries\n",
      "\n",
      "\n",
      "File: prompts_setup1_k5.json\n",
      "  Found 1356 entries\n",
      "\n",
      "\n",
      "File: prompts_setup2_k10.json\n",
      "  Found 1356 entries\n",
      "\n",
      "\n",
      "File: prompts_setup2_k5.json\n",
      "  Found 1356 entries\n",
      "\n",
      "\n",
      "Processing dataset: fir\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: prompts_setup1_k10.json\n",
      "  Found 1001 entries\n",
      "\n",
      "\n",
      "File: prompts_setup1_k5.json\n",
      "  Found 1001 entries\n",
      "\n",
      "\n",
      "File: prompts_setup2_k10.json\n",
      "  Found 1001 entries\n",
      "\n",
      "\n",
      "File: prompts_setup2_k5.json\n",
      "  Found 1001 entries\n",
      "\n",
      "\n",
      "Processing dataset: fir_hash\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: prompts_setup1_k10.json\n",
      "  Found 570 entries\n",
      "\n",
      "\n",
      "File: prompts_setup1_k5.json\n",
      "  Found 570 entries\n",
      "\n",
      "\n",
      "File: prompts_setup2_k10.json\n",
      "  Found 570 entries\n",
      "\n",
      "\n",
      "File: prompts_setup2_k5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 570 entries\n",
      "\n",
      "Loading model: meta-llama/Llama-3.3-70B-Instruct\n",
      "Model path: /scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b\n",
      "Using 2 GPUs\n",
      "INFO 12-31 06:53:38 config.py:542] This model supports multiple tasks: {'embed', 'score', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 12-31 06:53:38 config.py:1401] Defaulting to use mp for distributed inference\n",
      "INFO 12-31 06:53:38 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b', speculative_config=None, tokenizer='/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=30000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 12-31 06:53:39 multiproc_worker_utils.py:300] Reducing Torch parallelism from 6 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-31 06:53:39 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 06:53:39 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 12-31 06:53:40 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 06:53:41 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 12-31 06:53:42 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 12-31 06:53:42 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 06:53:42 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 06:53:42 pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1231 06:53:42.804634222 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "[W1231 06:53:42.811383420 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-31 06:53:43 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/asing725/.cache/vllm/gpu_p2p_access_cache_for_0,1,2.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 06:53:43 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/asing725/.cache/vllm/gpu_p2p_access_cache_for_0,1,2.json\n",
      "INFO 12-31 06:53:43 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_29dd9b9d'), local_subscribe_port=37625, remote_subscribe_port=None)\n",
      "INFO 12-31 06:53:43 model_runner.py:1110] Starting to load model /scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 06:53:43 model_runner.py:1110] Starting to load model /scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a64668de87f47368cdd86ced950e6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 07:07:54 model_runner.py:1115] Loading model weights took 65.7409 GB\n",
      "INFO 12-31 07:07:54 model_runner.py:1115] Loading model weights took 65.7409 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m /home/asing725/.conda/envs/ranker/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:725: UserWarning: Graph break due to unsupported builtin builtins.__import__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m   torch._dynamo.utils.warn_once(msg)\n",
      "/home/asing725/.conda/envs/ranker/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:725: UserWarning: Graph break due to unsupported builtin builtins.__import__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
      "  torch._dynamo.utils.warn_once(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-31 07:08:08 worker.py:267] Memory profiling takes 13.48 seconds\n",
      "INFO 12-31 07:08:08 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.97) = 76.87GiB\n",
      "INFO 12-31 07:08:08 worker.py:267] model weights take 65.74GiB; non_torch_memory takes 0.65GiB; PyTorch activation peak memory takes 3.79GiB; the rest of the memory reserved for KV Cache is 6.70GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 07:08:08 worker.py:267] Memory profiling takes 13.46 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 07:08:08 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.97) = 76.87GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 07:08:08 worker.py:267] model weights take 65.74GiB; non_torch_memory takes 0.65GiB; PyTorch activation peak memory takes 3.79GiB; the rest of the memory reserved for KV Cache is 6.70GiB.\n",
      "INFO 12-31 07:08:08 executor_base.py:110] # CUDA blocks: 2742, # CPU blocks: 1638\n",
      "INFO 12-31 07:08:08 executor_base.py:115] Maximum concurrency for 30000 tokens per request: 1.46x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2889447)\u001b[0;0m INFO 12-31 07:08:12 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-31 07:08:12 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:25<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: Cuda error /workspace/csrc/custom_all_reduce.cuh:368 'invalid argument'\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"/scratch/asing725/CSE336/privacy_qa/all_prompts\"\n",
    "\n",
    "process_prompts(BASE_PATH, model_name=\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "# process_prompts(BASE_PATH, model_name=\"openai/gpt-oss-120b\")\n",
    "# process_prompts(BASE_PATH, model_name=\"Qwen/Qwen2.5-72B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd567b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
