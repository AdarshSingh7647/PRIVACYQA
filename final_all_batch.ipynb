{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c6692",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "NUM_GPUS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 512\n",
    "batch_size = 200\n",
    "\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List\n",
    "from itertools import islice\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def run_hf_inference(\n",
    "    prompts_dict,\n",
    "    model_name: str,\n",
    "    model_path,\n",
    "    temperature: float = 0.01,\n",
    "    max_tokens: int = MAX_TOKENS,\n",
    "    tensor_parallel_size: int = NUM_GPUS,\n",
    "    **sampling_kwargs):\n",
    "    \"\"\"\n",
    "    Run inference on a dictionary of prompts using HuggingFace Transformers.\n",
    "    \n",
    "    Args:\n",
    "        prompts_dict: Dictionary mapping output_file -> list of prompts\n",
    "        model_name: Name of the model (key in model_configs)\n",
    "        model_path: Path to the model\n",
    "        temperature: Sampling temperature\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        tensor_parallel_size: Number of GPUs to use\n",
    "        **sampling_kwargs: Additional sampling parameters\n",
    "    \n",
    "    Returns:\n",
    "        None (saves results to files)\n",
    "    \"\"\"    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(f\"Model path: {model_path}\")\n",
    "    print(f\"Using {tensor_parallel_size} GPUs\")\n",
    "    \n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        dtype=\"auto\",\n",
    "        tensor_parallel_size=NUM_GPUS,\n",
    "        # pipeline_parallel_size=NUM_GPUS,\n",
    "        trust_remote_code=True,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        # max_model_len=50000,\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.01,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "    # Flatten all prompts and track their sources\n",
    "    all_prompts = []\n",
    "    prompt_to_file = []\n",
    "    \n",
    "    for output_file, prompts in prompts_dict.items():\n",
    "        for prompt in prompts:\n",
    "            all_prompts.append(prompt)\n",
    "            prompt_to_file.append(output_file)\n",
    "    \n",
    "    print(f\"\\nRunning inference on {len(all_prompts)} prompts\")\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    file_results = {output_file: [] for output_file in prompts_dict.keys()}\n",
    "    file_counters = {output_file: 0 for output_file in prompts_dict.keys()}\n",
    "    \n",
    "    # Process prompts in batches\n",
    "    for i in tqdm(range(0, len(all_prompts), batch_size), desc=\"Inference Batches\"):\n",
    "        batch_prompts = all_prompts[i:i + batch_size]\n",
    "        batch_files = prompt_to_file[i:i + batch_size]\n",
    "        \n",
    "        # Generate texts\n",
    "        generated_texts = llm.generate(batch_prompts, sampling_params)\n",
    "        \n",
    "        # Collect results for this batch\n",
    "        for prompt, generated_text, output_file in zip(batch_prompts, generated_texts, batch_files):\n",
    "            result = {\n",
    "                \"prompt_question_index\": file_counters[output_file],\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": generated_text.outputs[0].text,\n",
    "                \"prompt_length\": len(generated_text.prompt_token_ids),\n",
    "                \"response_length\": len(generated_text.outputs[0].token_ids),\n",
    "                \"model\": model_name,\n",
    "            }\n",
    "            file_results[output_file].append(result)\n",
    "            file_counters[output_file] += 1\n",
    "    \n",
    "        # Save results to respective files\n",
    "        for output_file, results in file_results.items():\n",
    "            output_path = Path(output_file)\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "MODELS = {\n",
    "    # \"openai/gpt-oss-120b\": \"/scratch/asing725/Huggingface/hub/models--openai--gpt-oss-120b/snapshots/b5c939de8f754692c1647ca79fbf85e8c1e70f8a\",\n",
    "    \"meta-llama/Llama-3.1-70B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.1-70B-Instruct/snapshots/1605565b47bb9346c5515c34102e054115b4f98b\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31\",\n",
    "}\n",
    "\n",
    "def process_prompts(base_path: str, model_name: str):\n",
    "    base_dir = Path(base_path)\n",
    "\n",
    "    # Datasets to process\n",
    "    datasets = ['cfpb', 'fir', 'fir_hash']\n",
    "    \n",
    "    print(f\"Starting to process prompts from: {base_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Dictionary to store all prompts: output_file -> list of prompts\n",
    "    prompts_dict = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_dir = base_dir / dataset\n",
    "        \n",
    "        if not dataset_dir.exists():\n",
    "            print(f\"Skipping {dataset} - directory not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing dataset: {dataset}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get all JSON files\n",
    "        json_files = sorted(dataset_dir.glob(\"*.json\"))\n",
    "        for json_file in json_files:\n",
    "            if model_name == \"meta-llama/Llama-3.3-70B-Instruct\" and \"cfpb\"==dataset and \"prompts_setup1_k10\" in json_file.name:\n",
    "                print(f\"  Skipping file {json_file.name}  for model {model_name} and dataset {dataset}\")\n",
    "                continue\n",
    "            all_prompts = []\n",
    "            print(f\"\\nFile: {json_file.name}\")\n",
    "            \n",
    "            # Load JSON file\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle both single object and array\n",
    "            if isinstance(data, dict):\n",
    "                data = [data]\n",
    "            \n",
    "            print(f\"  Found {len(data)} entries\")\n",
    "            \n",
    "            # Process each entry\n",
    "            for idx, entry in enumerate(data, 1):\n",
    "                # Extract required fields                           \n",
    "                prompt = entry['prompt']\n",
    "                setup = entry['setup']\n",
    "                question_id = entry.get('base_question_id')\n",
    "                all_prompts.append(prompt)\n",
    "                \n",
    "                # print(f\"  Entry {idx}/{len(data)}: ID={question_id}, setup={setup}\")\n",
    "            \n",
    "            # Store prompts with their output file path\n",
    "            output_file = str(dataset_dir / json_file.name.replace(\"prompts\", f\"results_{model_name}\"))\n",
    "            prompts_dict[output_file] = all_prompts\n",
    "            print()\n",
    "    \n",
    "    # Single run_hf_inference call with all prompts\n",
    "    if prompts_dict:\n",
    "        run_hf_inference(prompts_dict, model_path=MODELS[model_name], model_name=model_name)\n",
    "                    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Processing complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/scratch/asing725/CSE336/privacy_qa/all_prompts\"\n",
    "# process_prompts(BASE_PATH, model_name=\"meta-llama/Llama-3.1-70B-Instruct\")\n",
    "process_prompts(BASE_PATH, model_name=MODEL_NAME)\n",
    "# process_prompts(BASE_PATH, model_name=\"Qwen/Qwen2.5-72B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd567b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
