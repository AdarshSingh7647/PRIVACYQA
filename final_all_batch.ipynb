{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf45feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asing725/.conda/envs/ranker/lib/python3.10/site-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e1ceedb1ba42e7b5804f771114139b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 37 files:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc3fd526c4c4e2a8f62cb79635537de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9702f0d9a348019e526798205705d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e885a74829a347798721bbcfdbeac582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e5cd45bca248038b3c7e87d176c4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3e3671b4c74c7c9ccdafca0a52ea42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "USAGE_POLICY:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9586dfc9ef6740b591a6d6a049b5c09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "metal/model.bin:   0%|          | 0.00/65.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d334f2078580448ab23bb1abe03dd397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00000-of-00014.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea1a78a1ba348dc88f4f0ba7bf6142b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00014.safetensors:   0%|          | 0.00/4.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d3f551ae634fbab626214776d76864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00014.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be536b7e7f514c86985afa017594f27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00014.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6417b74aafbe429daad019150df58708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00014.safetensors:   0%|          | 0.00/4.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed6d33815a34ecdb7a07854dd728168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00014.safetensors:   0%|          | 0.00/4.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b48841af00d41e88789e9dfd9193fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00014.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c3a8ebd8704c67b72ce3c07b9d5192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00014.safetensors:   0%|          | 0.00/4.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842c09b729204dd2bf62c37324f45e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00014.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fb4c945eff49509d1506bca3ecf047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00014.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8f2a32a604448fb8037cb35b811d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00014.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64be800f58e64ebaba95c4ef8b26d302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00014.safetensors:   0%|          | 0.00/4.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf71db43cb604ef091e0a7181c64d7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00014.safetensors:   0%|          | 0.00/4.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724ab0f67e0c4f8694e6dafbf660b633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00014.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f855af3d95ec4b86a26a129f7579a2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00014.safetensors:   0%|          | 0.00/4.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c425fc2e17474babebac2002cb3956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8bac461e6944268155568e50338d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/377 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e94d80bd2b41d69cdcc7ab5b7975f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dtypes.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a501b0e5f9344e4180c74cd0ed64eaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/model--00001-of-00007.safetenso(…):   0%|          | 0.00/10.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b5a70cb8034f61afd30bcd06edcbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/model--00002-of-00007.safetenso(…):   0%|          | 0.00/10.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01df22885b8a4a87a78644462208da3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/model--00003-of-00007.safetenso(…):   0%|          | 0.00/10.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbecc205333549cc88ddb8456f483c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/model--00004-of-00007.safetenso(…):   0%|          | 0.00/10.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d93e957f80c49c2a6e1fc1c0a96a813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/model--00005-of-00007.safetenso(…):   0%|          | 0.00/10.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778b26f892584e9bb1f6038e2cc12531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/model--00006-of-00007.safetenso(…):   0%|          | 0.00/10.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3e397939144fa8b19b3e66de1ad9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/model--00007-of-00007.safetenso(…):   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16594878f604967a482e0ae4671c1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/scratch/asing725/Huggingface/hub/openai-gpt-oss-120b'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# model_id = \"openai/gpt-oss-120b\"   # change to any HF model\n",
    "# local_dir = \"/scratch/asing725/Huggingface/hub/openai-gpt-oss-120b\"\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=model_id,\n",
    "#     local_dir=local_dir,\n",
    "#     local_dir_use_symlinks=False  # important for scratch/shared filesystems\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c6692",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "NUM_GPUS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 512\n",
    "batch_size = 200\n",
    "\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List\n",
    "from itertools import islice\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def run_hf_inference(\n",
    "    prompts_dict,\n",
    "    model_name: str,\n",
    "    model_path,\n",
    "    temperature: float = 0.01,\n",
    "    max_tokens: int = MAX_TOKENS,\n",
    "    tensor_parallel_size: int = NUM_GPUS,\n",
    "    **sampling_kwargs):\n",
    "    \"\"\"\n",
    "    Run inference on a dictionary of prompts using HuggingFace Transformers.\n",
    "    \n",
    "    Args:\n",
    "        prompts_dict: Dictionary mapping output_file -> list of prompts\n",
    "        model_name: Name of the model (key in model_configs)\n",
    "        model_path: Path to the model\n",
    "        temperature: Sampling temperature\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        tensor_parallel_size: Number of GPUs to use\n",
    "        **sampling_kwargs: Additional sampling parameters\n",
    "    \n",
    "    Returns:\n",
    "        None (saves results to files)\n",
    "    \"\"\"    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(f\"Model path: {model_path}\")\n",
    "    print(f\"Using {tensor_parallel_size} GPUs\")\n",
    "    \n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        dtype=\"auto\",\n",
    "        tensor_parallel_size=NUM_GPUS,\n",
    "        # pipeline_parallel_size=NUM_GPUS,\n",
    "        trust_remote_code=True,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        # max_model_len=50000,\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.01,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "    # Flatten all prompts and track their sources\n",
    "    all_prompts = []\n",
    "    prompt_to_file = []\n",
    "    \n",
    "    for output_file, prompts in prompts_dict.items():\n",
    "        for prompt in prompts:\n",
    "            all_prompts.append(prompt)\n",
    "            prompt_to_file.append(output_file)\n",
    "    \n",
    "    print(f\"\\nRunning inference on {len(all_prompts)} prompts\")\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    file_results = {output_file: [] for output_file in prompts_dict.keys()}\n",
    "    file_counters = {output_file: 0 for output_file in prompts_dict.keys()}\n",
    "    \n",
    "    # Process prompts in batches\n",
    "    for i in tqdm(range(0, len(all_prompts), batch_size), desc=\"Inference Batches\"):\n",
    "        batch_prompts = all_prompts[i:i + batch_size]\n",
    "        batch_files = prompt_to_file[i:i + batch_size]\n",
    "        \n",
    "        # Generate texts\n",
    "        generated_texts = llm.generate(batch_prompts, sampling_params)\n",
    "        \n",
    "        # Collect results for this batch\n",
    "        for prompt, generated_text, output_file in zip(batch_prompts, generated_texts, batch_files):\n",
    "            result = {\n",
    "                \"prompt_question_index\": file_counters[output_file],\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": generated_text.outputs[0].text,\n",
    "                \"prompt_length\": len(generated_text.prompt_token_ids),\n",
    "                \"response_length\": len(generated_text.outputs[0].token_ids),\n",
    "                \"model\": model_name,\n",
    "            }\n",
    "            file_results[output_file].append(result)\n",
    "            file_counters[output_file] += 1\n",
    "    \n",
    "        # Save results to respective files\n",
    "        for output_file, results in file_results.items():\n",
    "            output_path = Path(output_file)\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "MODELS = {\n",
    "    \"openai/gpt-oss-120b\": \"/scratch/asing725/Huggingface/hub/openai-gpt-oss-120b/\",\n",
    "    # \"meta-llama/Llama-3.1-70B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.1-70B-Instruct/snapshots/1605565b47bb9346c5515c34102e054115b4f98b\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b\",\n",
    "    # \"Qwen/Qwen2.5-72B-Instruct\": \"/scratch/asing725/Huggingface/hub/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31\",\n",
    "    \"Qwen/Qwen3-Next-80B-A3B-Instruct\": \"/scratch/asing725/Huggingface/hub/qwen3-next-80b-a3b-instruct\",\n",
    "}\n",
    "\n",
    "def process_prompts(base_path: str, model_name: str):\n",
    "    base_dir = Path(base_path)\n",
    "\n",
    "    # Datasets to process\n",
    "    datasets = ['fir','fir_hash']\n",
    "    \n",
    "    print(f\"Starting to process prompts from: {base_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Dictionary to store all prompts: output_file -> list of prompts\n",
    "    prompts_dict = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_dir = base_dir / dataset\n",
    "        \n",
    "        if not dataset_dir.exists():\n",
    "            print(f\"Skipping {dataset} - directory not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing dataset: {dataset}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get all JSON files\n",
    "        json_files = sorted(dataset_dir.glob(\"*.json\"))\n",
    "        for json_file in json_files:\n",
    "            # if model_name == \"meta-llama/Llama-3.3-70B-Instruct\" and \"cfpb\"==dataset and \"prompts_setup1_k10\" in json_file.name:\n",
    "            #     print(f\"  Skipping file {json_file.name}  for model {model_name} and dataset {dataset}\")\n",
    "            #     continue\n",
    "            all_prompts = []\n",
    "            print(f\"\\nFile: {json_file.name}\")\n",
    "            \n",
    "            # Load JSON file\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle both single object and array\n",
    "            if isinstance(data, dict):\n",
    "                data = [data]\n",
    "            \n",
    "            print(f\"  Found {len(data)} entries\")\n",
    "            \n",
    "            # Process each entry\n",
    "            for idx, entry in enumerate(data, 1):\n",
    "                # Extract required fields                           \n",
    "                prompt = entry['prompt']\n",
    "                setup = entry['setup']\n",
    "                question_id = entry.get('base_question_id')\n",
    "                all_prompts.append(prompt)\n",
    "                \n",
    "                # print(f\"  Entry {idx}/{len(data)}: ID={question_id}, setup={setup}\")\n",
    "            \n",
    "            # Store prompts with their output file path\n",
    "            output_file = str(dataset_dir / json_file.name.replace(\"prompts\", f\"results_{model_name}\"))\n",
    "            prompts_dict[output_file] = all_prompts\n",
    "            # print()\n",
    "    \n",
    "    # Single run_hf_inference call with all prompts\n",
    "    if prompts_dict:\n",
    "        run_hf_inference(prompts_dict, model_path=MODELS[model_name], model_name=model_name)\n",
    "                    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Processing complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/scratch/asing725/CSE336/privacy_qa/all_prompts\"\n",
    "# process_prompts(BASE_PATH, model_name=\"meta-llama/Llama-3.1-70B-Instruct\")\n",
    "process_prompts(BASE_PATH, model_name=MODEL_NAME)\n",
    "# process_prompts(BASE_PATH, model_name=\"Qwen/Qwen2.5-72B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd567b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
